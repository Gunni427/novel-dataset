{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, http://www.novelupdates.com/,\n",
    "then cleans the data and arrange everything into a dataset.\n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\"\n",
    "\n",
    "# For testing\n",
    "novels_max_pages = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. Therefore, these are gathered from existing list of novels. First the maximum number of novel pages is retrieved and then the novels on these are iterated to get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with novels: 194\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum number of pages with novel\n",
    "def get_novel_list_max_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    page_links = dig_pag.find_all('a')\n",
    "    last_page_link = str(page_links[2]) # The last page is the 3rd\n",
    "    num = re.search('pg=\\d+', last_page_link).group()[3:]\n",
    "    return int(num)\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "novels_max_pages = get_novel_list_max_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_max_pages))\n",
    "\n",
    "\n",
    "# Get all novel ids from the novel lists\n",
    "def get_novel_ids(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('table', attrs={'id':'myTable'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    novel_ids = []\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')[-1]\n",
    "        novel_id = col.a['id'][3:]\n",
    "        novel_ids.append(novel_id)\n",
    "    return novel_ids\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1,novels_max_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(element, check=lambda e: e.string, func=lambda e: e.string.strip(),\n",
    "              check2=lambda e: e.text, func2=lambda e: e.text.strip()):\n",
    "    if check(element) is None:\n",
    "        if check2(element) is None:\n",
    "            return None\n",
    "        return func2(element)\n",
    "    return func(element)\n",
    "\n",
    "\n",
    "def empty(element):\n",
    "    return get_value(element) == \"\"\n",
    "\n",
    "\n",
    "def get_bool(string):\n",
    "    '''possible values is Yes, No, N/A'''\n",
    "    if string == \"Yes\":\n",
    "        return True\n",
    "    elif string == \"No\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_info(content):\n",
    "    gen_info = {}\n",
    "    gen_info['name'] = get_value(content.div)\n",
    "    gen_info['assoc_names'] = list(content.find('div', attrs={'id': 'editassociated'}).stripped_strings)\n",
    "    gen_info['original_langauge'] = get_value(content.find('div', attrs={'id': 'showlang'}), \n",
    "                                          lambda e: e.a, \n",
    "                                          lambda e: e.text.strip().lower())\n",
    "    gen_info['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .find_all('a')]\n",
    "    gen_info['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    gen_info['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    return gen_info\n",
    "\n",
    "\n",
    "def publisher_info(content):\n",
    "    pub_info = {}\n",
    "    pub_info['start_year'] = get_value(content.find('div', attrs={'id': 'edityear'}),)\n",
    "    pub_info['licensed'] = get_bool(get_value(content.find('div', attrs={'id': 'showlicensed'})))\n",
    "    pub_info['original_publisher'] = get_value(content.find('div', attrs={'id': 'showopublisher'}),\n",
    "                                               lambda e: e.a, \n",
    "                                               lambda e: e.a.string.strip().lower())\n",
    "    pub_info['english_publisher'] = get_value(content.find('div', attrs={'id': 'showepublisher'}),\n",
    "                                              lambda e: e.a, \n",
    "                                              lambda e: e.a.string.strip().lower())\n",
    "    return pub_info\n",
    "\n",
    "\n",
    "def chapter_info(soup, content):\n",
    "    chap_info = {}\n",
    "    chapter_status = get_value(content.find('div', attrs={'id': 'editstatus'}))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(chapter_status)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if chapter_status is not None:    \n",
    "        chap_info['complete_original'] = 'complete' in chapter_status.lower()\n",
    "        chapter_current = re.search('(.*)\\(', chapter_status).group(1).strip()\n",
    "        chap_info['chapters_original_current'] = chapter_current if chapter_current != \"\" else None \n",
    "    chap_info['complete_translated'] = get_bool(get_value(content.find('div', attrs={'id': 'showtranslated'})))\n",
    "    \n",
    "    table = soup.find('table', attrs={'id': 'myTable'})\n",
    "    if table is not None:\n",
    "        release_table = table.find('tbody')\n",
    "        chap_info['chapter_latest_translated'] = release_table.find('tr').find_all('td')[2].a.string.strip()\n",
    "    return chap_info\n",
    "    \n",
    "    \n",
    "def release_info(content):\n",
    "    rel_info = {}\n",
    "    release_freq = content.find('h5', attrs={'class': 'seriesother'}, string='Release Frequency').next_sibling\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    \n",
    "    if not empty(release_freq):\n",
    "        rel_info['release_freq'] = float(re.search('\\d+\\.?\\d*', release_freq).group(0))\n",
    "        \n",
    "    rel_info['activity_week_rank'] = int(activity[0].string[1:])\n",
    "    rel_info['activity_month_rank'] = int(activity[1].string[1:])\n",
    "    rel_info['activity_all_time_rank'] = int(activity[2].string[1:])\n",
    "    return rel_info\n",
    "    \n",
    "\n",
    "def community_info(content):\n",
    "    comm_info = {}\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    comm_info['on_reading_lists'] = int(content.find('b', attrs={'class': 'rlist'}).string)\n",
    "    comm_info['reading_list_month_rank'] = int(activity[3].string[1:])\n",
    "    comm_info['reading_list_all_time_rank'] = int(activity[4].string[1:])\n",
    "    \n",
    "    # rating\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    comm_info['rating'] = float(rating_text[0][1:])\n",
    "    comm_info['rating_votes'] = int(rating_text[3])\n",
    "    return comm_info\n",
    "    \n",
    "    \n",
    "def relation_info(soup, content):\n",
    "    rel_info = {}\n",
    "    any_related = content.find('h5', attrs={'class': 'seriesother'}, string='Related Series').next_sibling\n",
    "    if \"N/A\" not in any_related:\n",
    "        related_series_first = any_related.next_sibling.get('id')[3:]\n",
    "        rel_info['related_series_ids'] = [related_series_first]\n",
    "\n",
    "    rel_info['recommended_series_ids'] = []    \n",
    "    for series in soup.find_all('a', attrs={'class': 'genre'}, recursive=False):\n",
    "        if series.has_attr('title'):\n",
    "            rel_info['recommended_series_ids'].append(series.get('id')[3:])\n",
    "        else:\n",
    "            rel_info['related_series_ids'].append(series.get('id')[3:])\n",
    "    return rel_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n",
      "2 Volumes (Completed)\n",
      "16166\n",
      "42 Chapters (Ongoing)\n",
      "14845\n",
      "(Completed)\n",
      "1175\n",
      "2 Volumes (Completed)\n",
      "1177\n",
      "4 Volumes (Completed)\n",
      "13038\n",
      "35 Chapters (Ongoing)\n",
      "2042\n",
      "23 Volumes (Ongoing)\n",
      "2 Volumes (Koryosha)\n",
      "6612\n",
      "6 Volumes / 157 Chapters (Completed)\n",
      "22136\n",
      "2 Volumes (Ongoing)\n",
      "9343\n",
      "2 LN Volumes (Ongoing)\n",
      "110 WN Chapters (Completed)\n",
      "8233\n",
      "408 WN Chapters (Ongoing)\n",
      "3 Extra Editions\n",
      "9 LN Volumes (Ongoing)\n",
      "7959\n",
      "891 Chapters (Completed)\n",
      "2696\n",
      "519 Chapters (Completed)\n",
      "5 Volumes LN (Ongoing)\n",
      "22022\n",
      "N/A\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a1562a56a308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-a1562a56a308>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-a1562a56a308>\u001b[0m in \u001b[0;36mparse_novel_page\u001b[0;34m(id_num)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneral_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublisher_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelease_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-1031248b1912>\u001b[0m in \u001b[0;36mchapter_info\u001b[0;34m(soup, content)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchapter_status\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mchap_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'complete_original'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'complete'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchapter_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mchapter_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(.*)\\('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mchap_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapters_original_current'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchapter_current\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchapter_current\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mchap_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'complete_translated'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'showtranslated'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "def parse_novel_page(id_num):\n",
    "    page = requests.get(novel_page + str(id_num))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    data = {}\n",
    "    data['id'] = int(id_num)\n",
    "    \n",
    "    #TODO\n",
    "    print(id_num)\n",
    "    \n",
    "    data.update(general_info(content))\n",
    "    data.update(publisher_info(content))\n",
    "    data.update(chapter_info(soup, content))\n",
    "    data.update(release_info(content))\n",
    "    data.update(community_info(content))\n",
    "    data.update(relation_info(soup, content))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = pd.merge(df, df.id.apply(lambda x: parse_novel_page(x)), left_index=True, right_index=True)\n",
    "df = df.id.apply(lambda x: parse_novel_page(x))\n",
    "print(df.head)\n",
    "#df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
