{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, http://www.novelupdates.com/,\n",
    "then cleans the data and arrange everything into a dataset.\n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\"\n",
    "\n",
    "# For testing\n",
    "novels_max_pages = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. Therefore, these are gathered from existing list of novels. First the maximum number of novel pages is retrieved and then the novels on these are iterated to get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum number of pages with novel\n",
    "def get_novel_list_max_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    page_links = dig_pag.find_all('a')\n",
    "    last_page_link = str(page_links[2]) # The last page is the 3rd\n",
    "    num = re.search('pg=\\d+', last_page_link).group()[3:]\n",
    "    return int(num)\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "novels_max_pages = get_novel_list_max_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_max_pages))\n",
    "\n",
    "\n",
    "# Get all novel ids from the novel lists\n",
    "def get_novel_ids(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('table', attrs={'id':'myTable'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    novel_ids = []\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')[-1]\n",
    "        novel_id = col.a['id'][3:]\n",
    "        novel_ids.append(novel_id)\n",
    "    return novel_ids\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1,novels_max_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing help functions\n",
    "\n",
    "def get_value(element, check=lambda e: e.string, func=lambda e: e.string.strip()):\n",
    "    if check(element) is None:\n",
    "        return None\n",
    "    return func(element)\n",
    "\n",
    "def get_bool(string):\n",
    "    '''possible values is Yes, No, N/A'''\n",
    "    if string == \"Yes\":\n",
    "        return True\n",
    "    elif string == \"No\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_info(page, )\n",
    "# return a dict ??\n",
    "\n",
    "def publisher_info(page, )\n",
    "\n",
    "def chapter_info(page, )\n",
    "\n",
    "def release_info(page, )\n",
    "\n",
    "def community_info(page, )\n",
    "# both rating / community\n",
    "\n",
    "def relation_info(page, )\n",
    "\n",
    "def parse_novel_page(id_num):\n",
    "    page = requests.get(novel_page + str(id_num))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    data = {}\n",
    "    \n",
    "    #TODO\n",
    "    print(id_num)\n",
    "    \n",
    "    # general information\n",
    "    data['id'] = int(id_num)\n",
    "    data['name'] = get_value(content.div)\n",
    "    data['assoc_names'] = list(content.find('div', attrs={'id': 'editassociated'}).stripped_strings)\n",
    "    data['original_langauge'] = get_value(content.find('div', attrs={'id': 'showlang'}), \n",
    "                                          lambda e: e.a, \n",
    "                                          lambda e: e.text.strip().lower())\n",
    "    data['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .find_all('a')]\n",
    "    data['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    data['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    \n",
    "    # publisher\n",
    "    data['start_year'] = get_value(content.find('div', attrs={'id': 'edityear'}),)\n",
    "    data['licensed'] = get_bool(get_value(content.find('div', attrs={'id': 'showlicensed'})))\n",
    "    data['original_publisher'] = get_value(content.find('div', attrs={'id': 'showopublisher'}),\n",
    "                                               lambda e: e.a, \n",
    "                                               lambda e: e.a.string.strip().lower())\n",
    "    data['english_publisher'] = get_value(content.find('div', attrs={'id': 'showepublisher'}),\n",
    "                                              lambda e: e.a, \n",
    "                                              lambda e: e.a.string.strip().lower())\n",
    "    \n",
    "    # chapters\n",
    "    chapter_status = get_value(content.find('div', attrs={'id': 'editstatus'}))\n",
    "    if chapter_status is not None:\n",
    "        data['complete_original'] = 'complete' in chapter_status.lower()\n",
    "        chapter_current = re.search('(.*)\\(', chapter_status).group(1).strip()\n",
    "        data['chapters_original_current'] = chapter_current if chapter_current != \"\" else None \n",
    "    data['complete_translated'] = get_bool(get_value(content.find('div', attrs={'id': 'showtranslated'})))\n",
    "    \n",
    "    table = soup.find('table', attrs={'id': 'myTable'})\n",
    "    if table is not None:\n",
    "        release_table = table.find('tbody')\n",
    "        data['chapter_latest_translated'] = release_table.find('tr').find_all('td')[2].a.string.strip()\n",
    "        \n",
    "    # current release activity\n",
    "    release_freq = content.find('h5', attrs={'class': 'seriesother'}, string='Release Frequency').next_sibling\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    data['release_freq'] = float(re.search('\\d+\\.?\\d*', release_freq).group(0))\n",
    "    data['activity_week_rank'] = int(activity[0].string[1:])\n",
    "    data['activity_month_rank'] = int(activity[1].string[1:])\n",
    "    data['activity_all_time_rank'] = int(activity[2].string[1:])\n",
    "    \n",
    "    # community\n",
    "    data['on_reading_lists'] = int(content.find('b', attrs={'class': 'rlist'}).string)\n",
    "    data['reading_list_month_rank'] = int(activity[3].string[1:])\n",
    "    data['reading_list_all_time_rank'] = int(activity[4].string[1:])\n",
    "    \n",
    "    # rating\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    data['rating'] = float(rating_text[0][1:])\n",
    "    data['rating_votes'] = int(rating_text[3])\n",
    "    \n",
    "    # relations\n",
    "    any_related = content.find('h5', attrs={'class': 'seriesother'}, string='Related Series').next_sibling\n",
    "    if \"N/A\" not in any_related:\n",
    "        related_series_first = any_related.next_sibling.get('id')[3:]\n",
    "        data['related_series_ids'] = [related_series_first]\n",
    "\n",
    "    data['recommended_series_ids'] = []    \n",
    "    for series in soup.find_all('a', attrs={'class': 'genre'}, recursive=False):\n",
    "        if series.has_attr('title'):\n",
    "            data['recommended_series_ids'].append(series.get('id')[3:])\n",
    "        else:\n",
    "            data['related_series_ids'].append(series.get('id')[3:])\n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n",
      "2 Volumes\n",
      "16166\n",
      "42 Chapters\n",
      "14845\n",
      "None\n",
      "1175\n",
      "2 Volumes\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-2b249bf43fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df.to_csv('novels.csv', header=True, index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-2b249bf43fb2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df.to_csv('novels.csv', header=True, index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-6e300b8eee18>\u001b[0m in \u001b[0;36mparse_novel_page\u001b[0;34m(id_num)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mrelease_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'seriesother'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Release Frequency'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_sibling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'userrate rank'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_freq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\d+\\.?\\d*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelease_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activity_week_rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activity_month_rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, df.id.apply(lambda x: parse_novel_page(x)), left_index=True, right_index=True)\n",
    "df = df.id.apply(lambda x: parse_novel_page(x))\n",
    "print(df.head)\n",
    "#df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
