{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, http://www.novelupdates.com/,\n",
    "then cleans the data and arrange everything into a dataset.\n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. Therefore, these are gathered from existing list of novels. First the maximum number of novel pages is retrieved and then the novels on these are iterated to get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with novels: 155\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum number of pages with novel\n",
    "def get_novel_list_max_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    page_links = dig_pag.find_all('a')\n",
    "    last_page_link = str(page_links[2]) # The last page is the 3rd\n",
    "    num = re.search('pg=\\d+', last_page_link).group()[3:]\n",
    "    return int(num)\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "novels_max_pages = get_novel_list_max_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_max_pages))\n",
    "\n",
    "# For testing\n",
    "novels_max_pages = 2\n",
    "\n",
    "# Get all novel ids from the novel lists\n",
    "def get_novel_ids(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('table', attrs={'id':'myTable'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    novel_ids = []\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')[-1]\n",
    "        novel_id = col.a['id'][3:]\n",
    "        novel_ids.append(novel_id)\n",
    "    return novel_ids\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1,novels_max_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-7785edfd9453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#df = parse_novel_page('http://www.novelupdates.com/series/i-shall-seal-the-heavens/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-7785edfd9453>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#df = parse_novel_page('http://www.novelupdates.com/series/i-shall-seal-the-heavens/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_novel_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-7785edfd9453>\u001b[0m in \u001b[0;36mparse_novel_page\u001b[0;34m(id_num)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# chapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mchapter_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'editstatus'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mchapter_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'myTable'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tbody'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mcomplete_translated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'showtranslated'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "def parse_novel_page(id_num):\n",
    "    page = requests.get(novel_page + str(id_num))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    data = {}\n",
    "    \n",
    "    #TODO\n",
    "    print(id_num)\n",
    "    \n",
    "    # general information\n",
    "    data['id'] = int(id_num)\n",
    "    data['name'] = content.div.string.strip()\n",
    "    data['assoc_names'] = list(content\n",
    "                               .find('div', attrs={'id': 'editassociated'})\n",
    "                               .stripped_strings)\n",
    "    data['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .findAll('a')]\n",
    "    data['org_langauge'] = content.find('div', attrs={'id': 'showlang'}).a.text\n",
    "    data['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    data['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    \n",
    "    # publisher\n",
    "    data['start_year'] = int(content.find('div', attrs={'id': 'edityear'}).string.strip())\n",
    "    data['licensed'] = True if content.find('div', attrs={'id': 'showlicensed'}).string.strip() == 'Yes' else False\n",
    "    data['original_publisher'] = content.find('div', attrs={'id': 'showopublisher'}).a.string.strip().lower()\n",
    "    data['english_publisher'] = content.find('div', attrs={'id': 'showepublisher'}).a.string.strip().lower()\n",
    "\n",
    "    # chapters\n",
    "    chapter_status = content.find('div', attrs={'id': 'editstatus'}).string\n",
    "    chapter_table = soup.find('table', attrs={'id': 'myTable'}).find('tbody')\n",
    "    complete_translated = content.find('div', attrs={'id': 'showtranslated'}).a.string.strip()\n",
    "    \n",
    "    data['chapters_original_current'] = int(re.search('\\d+ Chapters', chapter_status).group(0).split(' ')[0])\n",
    "    data['chapter_latest_translated'] = chapter_table.find('tr').find_all('td')[2].a.string.strip()\n",
    "    data['complete_original'] = 'complete' in chapter_status.string.lower()\n",
    "    data['complete_translated'] = True if  complete_translated == 'Yes' else False\n",
    "    \n",
    "    # current release activity\n",
    "    release_freq = content.find('h5', attrs={'class': 'seriesother'}, string='Release Frequency').next_sibling\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    data['release_freq'] = float(re.search('\\d+\\.?\\d*', release_freq).group(0))\n",
    "    data['activity_week_rank'] = int(activity[0].string[1:])\n",
    "    data['activity_month_rank'] = int(activity[1].string[1:])\n",
    "    data['activity_all_time_rank'] = int(activity[2].string[1:])\n",
    "    \n",
    "    # community\n",
    "    data['on_reading_lists'] = int(content.find('b', attrs={'class': 'rlist'}).string)\n",
    "    data['reading_list_month_rank'] = int(activity[3].string[1:])\n",
    "    data['reading_list_all_time_rank'] = int(activity[4].string[1:])\n",
    "    \n",
    "    # rating\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    data['rating'] = float(rating_text[0][1:])\n",
    "    data['rating_votes'] = int(rating_text[3])\n",
    "    \n",
    "    # relations\n",
    "    related_series_first = content.find('h5', attrs={'class': 'seriesother'}, string='Related Series')\\\n",
    "        .next_sibling.next_sibling.get('id')[3:]\n",
    "    related_series_others = []\n",
    "    recommended_series_ids = []\n",
    "    for series in soup.find_all('a', attrs={'class': 'genre'}, recursive=False):\n",
    "        if series.has_attr('title'):\n",
    "            recommended_series_ids.append(series.get('id')[3:])\n",
    "        else:\n",
    "            related_series_others.append(series.get('id')[3:])\n",
    "    data['related_series_ids'] = [related_series_first] + related_series_others\n",
    "    data['recommended_series_ids'] = recommended_series_ids\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)\n",
    "\n",
    "#df = parse_novel_page('http://www.novelupdates.com/series/i-shall-seal-the-heavens/')\n",
    "\n",
    "df = pd.merge(df, df.id.apply(lambda x: parse_novel_page(x)), left_index=True, right_index=True)\n",
    "df = df.id.apply(lambda x: parse_novel_page(x))\n",
    "print(df.head)\n",
    "#df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
