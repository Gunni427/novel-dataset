{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, http://www.novelupdates.com/,\n",
    "then cleans the data and arrange everything into a dataset.\n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. Therefore, these are gathered from existing list of novels. First the maximum number of novel pages is retrieved and then the novels on these are iterated to get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with novels: 146\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum number of pages with novel\n",
    "def get_novel_list_max_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    page_links = dig_pag.find_all('a')\n",
    "    last_page_link = str(page_links[2]) # The last page is the 3rd\n",
    "    num = re.search('pg=\\d+', last_page_link).group()[3:]\n",
    "    return int(num)\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "novels_max_pages = get_novel_list_max_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_max_pages))\n",
    "\n",
    "# For testing\n",
    "novels_max_pages = 2\n",
    "\n",
    "# Get all novel ids from the novel lists\n",
    "def get_novel_ids(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('table', attrs={'id':'myTable'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    novel_ids = []\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')[-1]\n",
    "        novel_id = col.a['id'][3:]\n",
    "        novel_ids.append(novel_id)\n",
    "    return novel_ids\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1,novels_max_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-796c785348e3>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-796c785348e3>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    volumes_total =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def parse_novel_page(id_num):\n",
    "    #page = requests.get(novel_page + str(id_num))\n",
    "    page = requests.get(id_num) ## TESTING\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    data = {}\n",
    "    \n",
    "    # generatl information\n",
    "    data['id'] = int(id_num)\n",
    "    data['name'] = content.div.string.strip()\n",
    "    data['assoc_names'] = list(content\n",
    "                               .find('div', attrs={'id': 'editassociated'})\n",
    "                               .stripped_strings)\n",
    "    data['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .findAll('a')]\n",
    "    data['org_langauge'] = content.find('div', attrs={'id': 'showlang'}).a.text\n",
    "    data['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    data['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    \n",
    "    # publisher\n",
    "    data['start_year'] = int(content.find('div', attrs={'id': 'edityear'}).string.strip())\n",
    "    data['licensed'] = True if content.find('div', attrs={'id': 'showlicensed'}).string.strip() == 'Yes' else False\n",
    "    data['original_publisher'] = content.find('div', attrs={'id': 'showopublisher'}).a.string.strip().lower()\n",
    "    data['english_publisher'] = content.find('div', attrs={'id': 'showepublisher'}).a.string.strip().lower()\n",
    "\n",
    "    # chapters\n",
    "    volumes_total =\n",
    "    print(volumes_total)\n",
    "    #chapters_total =\n",
    "    #chapters_translated =\n",
    "    #complete\n",
    "    #complete_translated =\n",
    "    \n",
    "    # translated releases\n",
    "    #release_freq = \n",
    "    #activity_week_rank =\n",
    "    #activity_month_rank =\n",
    "    #activity_all_time_rank =\n",
    "    \n",
    "    # community\n",
    "    #on_reading_lists =\n",
    "    #reading_list_month_rank =\n",
    "    #reading_list_all_time_rank = \n",
    "\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    data['rating'] = float(rating_text[0][1:])\n",
    "    data['rating_votes'] = int(rating_text[3])\n",
    "    \n",
    "    \n",
    "    # relations\n",
    "    #related_series_ids =\n",
    "    #recommended_series_ids =\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = parse_novel_page('http://www.novelupdates.com/series/i-shall-seal-the-heavens/')\n",
    "\n",
    "#df = pd.merge(df, df.id.apply(lambda x: parse_novel_page(x)), left_index=True, right_index=True)\n",
    "#df = df.id.apply(lambda x: parse_novel_page(x))\n",
    "print(df.head)\n",
    "#df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
