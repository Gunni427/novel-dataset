{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, http://www.novelupdates.com/,\n",
    "then cleans the data and arrange everything into a dataset.\n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\"\n",
    "\n",
    "# For testing - maximum pages with novels to query\n",
    "novels_max_pages = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. Therefore, these are gathered from existing list of novels. First the maximum number of novel pages is retrieved and then the novels on these are iterated to get the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with novels: 212\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum number of pages with novels\n",
    "def get_novel_list_max_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    max_page = max([int(a.text) for a in dig_pag.find_all('a') if a.text.isdigit()])\n",
    "    return max_page\n",
    "\n",
    "# Get all novel ids from a single page\n",
    "def get_novel_ids(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('div', attrs={'class':'w-blog-content other'})\n",
    "    novels = table.find_all('div', attrs={'class': 'search_title'})\n",
    "    novel_ids = [novel.find('span', attrs={'class': 'rl_icons_en'}).get('id')[3:] for novel in novels] \n",
    "    return novel_ids\n",
    "\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "novels_max_pages = get_novel_list_max_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_max_pages))\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1,novels_max_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(element, check=lambda e: e.string, parse=lambda e: e.string.strip()):\n",
    "    if check(element) is None:\n",
    "        return None\n",
    "    pe = parse(element)\n",
    "    if ''.join(pe) == 'N/A':\n",
    "        return None\n",
    "    return pe\n",
    "\n",
    "              \n",
    "def get_value_str_txt(element, check_str=lambda e: e.string, parse_str=lambda e: e.string.strip(),\n",
    "                      check_txt=lambda e: e.text, parse_txt=lambda e: e.text.strip()):\n",
    "    res_str = get_value(element, check_str, parse_str)\n",
    "    res_txt = get_value(element, check_txt, parse_txt)\n",
    "    return res_str or res_txt\n",
    "              \n",
    "\n",
    "def empty(element):\n",
    "    return get_value(element) == \"\"\n",
    "\n",
    "\n",
    "def get_bool(string):\n",
    "    '''possible values is Yes, No, N/A'''\n",
    "    if string == \"Yes\":\n",
    "        return True\n",
    "    elif string == \"No\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_info(content):\n",
    "    gen_info = {}\n",
    "    gen_info['name'] = get_value(content.div)\n",
    "    gen_info['assoc_names'] = get_value(content.find('div', attrs={'id': 'editassociated'}), \n",
    "                                        check=lambda e: e, parse=lambda e: list(e.stripped_strings))\n",
    "    gen_info['original_langauge'] = get_value(content.find('div', attrs={'id': 'showlang'}), \n",
    "                                          lambda e: e.a, \n",
    "                                          lambda e: e.text.strip().lower())\n",
    "    gen_info['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .find_all('a')]\n",
    "    gen_info['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    gen_info['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    return gen_info\n",
    "\n",
    "\n",
    "def publisher_info(content):\n",
    "    pub_info = {}\n",
    "    pub_info['start_year'] = get_value(content.find('div', attrs={'id': 'edityear'}),)\n",
    "    pub_info['licensed'] = get_bool(get_value(content.find('div', attrs={'id': 'showlicensed'})))\n",
    "    pub_info['original_publisher'] = get_value(content.find('div', attrs={'id': 'showopublisher'}),\n",
    "                                               lambda e: e.a, \n",
    "                                               lambda e: e.a.string.strip().lower())\n",
    "    pub_info['english_publisher'] = get_value(content.find('div', attrs={'id': 'showepublisher'}),\n",
    "                                              lambda e: e.a, \n",
    "                                              lambda e: e.a.string.strip().lower())\n",
    "    return pub_info\n",
    "\n",
    "\n",
    "def chapter_info(soup, content):\n",
    "    chap_info = {}\n",
    "    chapter_status = get_value_str_txt(content.find('div', attrs={'id': 'editstatus'}))    \n",
    "    if chapter_status is not None:    \n",
    "        chap_info['complete_original'] = 'complete' in chapter_status.lower()\n",
    "        chapter_current = re.search('([^\\+\\(])+', chapter_status).group(1).strip()\n",
    "        chap_info['chapters_original_current'] = chapter_current if chapter_current != \"\" else None \n",
    "    chap_info['complete_translated'] = get_bool(get_value(content.find('div', attrs={'id': 'showtranslated'})))\n",
    "    \n",
    "    table = soup.find('table', attrs={'id': 'myTable'})\n",
    "    if table is not None:\n",
    "        release_table = table.find('tbody')\n",
    "        chap_info['chapter_latest_translated'] = release_table.find('tr').find_all('td')[2].a.string.strip()\n",
    "    return chap_info\n",
    "    \n",
    "    \n",
    "def release_info(content):\n",
    "    rel_info = {}\n",
    "    release_freq = content.find('h5', attrs={'class': 'seriesother'}, string='Release Frequency').next_sibling\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    \n",
    "    if not empty(release_freq):\n",
    "        rel_info['release_freq'] = float(re.search('\\d+\\.?\\d*', release_freq).group(0))\n",
    "        \n",
    "    rel_info['activity_week_rank'] = int(activity[0].string[1:])\n",
    "    rel_info['activity_month_rank'] = int(activity[1].string[1:])\n",
    "    rel_info['activity_all_time_rank'] = int(activity[2].string[1:])\n",
    "    return rel_info\n",
    "    \n",
    "\n",
    "def community_info(content):\n",
    "    comm_info = {}\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    comm_info['on_reading_lists'] = int(content.find('b', attrs={'class': 'rlist'}).string)\n",
    "    comm_info['reading_list_month_rank'] = int(activity[3].string[1:])\n",
    "    comm_info['reading_list_all_time_rank'] = int(activity[4].string[1:])\n",
    "    \n",
    "    # rating\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    comm_info['rating'] = float(rating_text[0][1:])\n",
    "    comm_info['rating_votes'] = int(rating_text[3])\n",
    "    return comm_info\n",
    "    \n",
    "    \n",
    "def relation_info(soup, content):\n",
    "    rel_info = {}\n",
    "    any_related = content.find('h5', attrs={'class': 'seriesother'}, string='Related Series').next_sibling\n",
    "    if \"N/A\" not in any_related:\n",
    "        related_series_first = any_related.next_sibling.get('id')[3:]\n",
    "        rel_info['related_series_ids'] = [related_series_first]\n",
    "\n",
    "    rel_info['recommended_series_ids'] = []    \n",
    "    for series in soup.find_all('a', attrs={'class': 'genre'}, recursive=False):\n",
    "        if series.has_attr('title'):\n",
    "            rel_info['recommended_series_ids'].append(series.get('id')[3:])\n",
    "        else:\n",
    "            rel_info['related_series_ids'].append(series.get('id')[3:])\n",
    "    return rel_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_novel_page(id_num):\n",
    "    page = requests.get(novel_page + str(id_num))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    data = {}\n",
    "    data['id'] = int(id_num)\n",
    "    \n",
    "    #TODO\n",
    "    print(id_num)\n",
    "    \n",
    "    data.update(general_info(content))\n",
    "    data.update(publisher_info(content))\n",
    "    data.update(chapter_info(soup, content))\n",
    "    data.update(release_info(content))\n",
    "    data.update(community_info(content))\n",
    "    data.update(relation_info(soup, content))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = pd.merge(df, df.id.apply(lambda x: parse_novel_page(x)), left_index=True, right_index=True)\n",
    "df = df.id.apply(lambda x: parse_novel_page(x))\n",
    "print(df.head)\n",
    "#df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
