{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel dataset web scraper\n",
    "\n",
    "Gathers novel information from novelupdates, i.e. http://www.novelupdates.com/.  \n",
    "The data is then cleaned and arranged into a dataset.  \n",
    "The dataset is finally saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Display all columns when showing dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "novel_list_page = \"http://www.novelupdates.com/novelslisting/?st=1&pg=\"\n",
    "novel_page = \"http://www.novelupdates.com/?p=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be an easy way to get all novel ids. These ids do not seem to be necessarily be strictly consecutive or increasing. Hence a brute force method is used to gather the ids of all current novels.\n",
    "\n",
    "The ids are gathered from a list of all the novels. \n",
    "The list contains mulitple pages/tabs with each page consisting of 25 novels.\n",
    "First the number of novel pages is retrieved and then the pages are iterated though in order to scrape the ids of the novels on each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with novels: 1\n"
     ]
    }
   ],
   "source": [
    "def get_novel_list_num_pages(page):\n",
    "    \"\"\"\n",
    "    Get the maximum number of pages with novels.\n",
    "    This number is not contant since the number of novels on the website are increasing.\n",
    "    Following the current website layout each page have 25 novels.\n",
    "    \n",
    "    :param page: The web address to the novel list, presumably the first page but can be any.\n",
    "    :returns: An int representing the current number of pages of the novel lists.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dig_pag = soup.find('div', attrs={'class':'digg_pagination'})\n",
    "    max_page = max([int(a.text) for a in dig_pag.find_all('a') if a.text.isdigit()])\n",
    "    return max_page\n",
    "\n",
    "# Get all novel ids from a single page\n",
    "def get_novel_ids(page):\n",
    "    \"\"\"\n",
    "    Gets all the novel ids from a page.\n",
    "    \n",
    "    :param page: One of the pages with novels.\n",
    "    :returns: A list with all novel ids for the novels on the page.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find('div', attrs={'class':'w-blog-content other'})\n",
    "    novels = table.find_all('div', attrs={'class': 'search_title'})\n",
    "    novel_ids = [novel.find('span', attrs={'class': 'rl_icons_en'}).get('id')[3:] for novel in novels]\n",
    "    novel_ids = [int(n) for n in novel_ids]\n",
    "    return novel_ids\n",
    "\n",
    "\n",
    "page = requests.get(novel_list_page + '1')\n",
    "\n",
    "# TOOD: For testing - only use 2 pages for now.\n",
    "novels_num_pages = 1\n",
    "#novels_max_pages = get_novel_list_num_pages(page)\n",
    "print(\"Pages with novels: \" + str(novels_num_pages))\n",
    "\n",
    "all_novel_ids = []\n",
    "for i in range(1, novels_num_pages+1):\n",
    "    page = requests.get(novel_list_page + str(i))\n",
    "    novel_ids = get_novel_ids(page)\n",
    "    all_novel_ids.extend(novel_ids)\n",
    "    time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_novel_ids, columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(element, check=lambda e: e.string, parse=lambda e: e.string.strip()):\n",
    "    \"\"\"\n",
    "    Gets the value of a HTML element/node following the parse function. \n",
    "    This function is necessary since the novel pages are not always consistent with each other. \n",
    "    Also checks if the value is 'N/A' and returns None in that case.\n",
    "    \n",
    "    :param element: A HTML element/node.\n",
    "    :param check: A function to be applied on the element. \n",
    "                  Checks if the element object have a retrun value for the function or is it's None.\n",
    "    :param parse: A function to parse the element if it passes the check.\n",
    "    :returns: The value returned by running the parse function on the element.\n",
    "              None is returned if the element does not pass the check function or if the value is 'N/A'.\n",
    "    \"\"\"\n",
    "    if check(element) is None:\n",
    "        return None\n",
    "    pe = parse(element)\n",
    "    if ''.join(pe) == 'N/A':\n",
    "        return None\n",
    "    return pe\n",
    "\n",
    "              \n",
    "def get_value_str_txt(element, check_one=lambda e: e.string, parse_one=lambda e: e.string.strip(),\n",
    "                      check_two=lambda e: e.text, parse_two=lambda e: e.text.strip()):\n",
    "    \"\"\"\n",
    "    Used when it's unknown which function to apply on an element to obtain it's value.\n",
    "    For example, if .string or .text should be used.\n",
    "    The functions are applied in order, if the first one returns None then the second one is tried.\n",
    "    \n",
    "    :param element: A HTML element/node.\n",
    "    :param check_one: A function to be applied on the element.\n",
    "                      Checks if the element object have a retrun value for the function or is it's None.\n",
    "    :param parse_one: A function to parse the element if it passes the check.\n",
    "    :param check_two: A function to be applied on the element.\n",
    "                      Checks if the element object have a retrun value for the function or is it's None.\n",
    "    :param parse_two: A function to parse the element if it passes the check.\n",
    "    :returns: The value returned by running parse_one or parse_two on the element.\n",
    "    \"\"\"\n",
    "    res_one = get_value(element, check_one, parse_one)\n",
    "    res_two = get_value(element, check_two, parse_two)\n",
    "    return res_one or res_two\n",
    "              \n",
    "\n",
    "def empty(element):\n",
    "    \"\"\"\n",
    "    Checks if running .string on the element returns an empty string.\n",
    "    \n",
    "    :param element: A HTML element/node.\n",
    "    :returns: A boolean representing whether the element contains an empty string.\n",
    "    \"\"\"\n",
    "    return get_value(element) == \"\"\n",
    "\n",
    "\n",
    "def get_bool(string):\n",
    "    \"\"\"\n",
    "    Convinience function to convert a string to a boolean.\n",
    "    Handles Yes, yes, No and no.\n",
    "    \n",
    "    :param string: String to convert to boolean.\n",
    "    :retruns: The boolean representation of the string or None.\n",
    "    \"\"\"\n",
    "    if string is None:\n",
    "        return None\n",
    "    \n",
    "    if string.lower() == \"yes\":\n",
    "        return True\n",
    "    elif string.lower() == \"no\":\n",
    "        return False\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all general information of a specific novel.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    \n",
    "    gen_info = {}\n",
    "    gen_info['name'] = get_value(content.find('div', attrs={'class', 'seriestitlenu'}))\n",
    "    gen_info['assoc_names'] = get_value(content.find('div', attrs={'id': 'editassociated'}), \n",
    "                                        check=lambda e: e, parse=lambda e: list(e.stripped_strings))\n",
    "    gen_info['original_langauge'] = get_value(content.find('div', attrs={'id': 'showlang'}), \n",
    "                                          lambda e: e.a, \n",
    "                                          lambda e: e.text.strip().lower())\n",
    "    gen_info['authors'] = [author.text.lower()\n",
    "                for author in content\n",
    "                  .find('div', attrs={'id': 'showauthors'})\n",
    "                  .find_all('a')]\n",
    "    gen_info['genres'] = [genre.text.lower()\n",
    "                for genre in content\n",
    "                  .find('div', attrs={'id': 'seriesgenre'})\n",
    "                  .find_all('a', attrs={'class': 'genre'})]\n",
    "    gen_info['tags'] = [tag.text.lower()\n",
    "                for tag in content\n",
    "                  .find('div', attrs={'id': 'showtags'})\n",
    "                  .find_all('a')]\n",
    "    return gen_info\n",
    "\n",
    "\n",
    "def publisher_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all publisher information of a specific novel.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    pub_info = {}\n",
    "    pub_info['start_year'] = get_value(content.find('div', attrs={'id': 'edityear'}),)\n",
    "    pub_info['licensed'] = get_bool(get_value(content.find('div', attrs={'id': 'showlicensed'})))\n",
    "    pub_info['original_publisher'] = get_value(content.find('div', attrs={'id': 'showopublisher'}),\n",
    "                                               lambda e: e.a, \n",
    "                                               lambda e: e.a.string.strip().lower())\n",
    "    pub_info['english_publisher'] = get_value(content.find('div', attrs={'id': 'showepublisher'}),\n",
    "                                              lambda e: e.a, \n",
    "                                              lambda e: e.a.string.strip().lower())\n",
    "    return pub_info\n",
    "\n",
    "\n",
    "def chapter_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all chapter information of a specific novel. \n",
    "    Both latest released chapters and if the novel is complete.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    chap_info = {}\n",
    "    chapter_status = get_value_str_txt(content.find('div', attrs={'id': 'editstatus'}))\n",
    "    \n",
    "    if chapter_status is not None:\n",
    "        chap_info['complete_original'] = 'complete' in chapter_status.lower()\n",
    "        chapter_current = re.search('(\\d+)[ wnl]*(?=chap)', chapter_status.lower())\n",
    "        if chapter_current is not None:\n",
    "            chapter_current = chapter_current.group(1).strip() + \" chapters\"\n",
    "        else:    \n",
    "            # Check if volume\n",
    "            chapter_current = re.search('(\\d+)[ wnl]*(?=volu)', chapter_status.lower())\n",
    "            if chapter_current is not None:\n",
    "                chapter_current = chapter_current.group(1).strip() + \" volumes\"\n",
    "            else:\n",
    "                # Get the first number\n",
    "                chapter_current = re.search('(\\d+)', chapter_status.lower())\n",
    "                if chapter_current is not None:\n",
    "                    chapter_current = chapter_current.group(1).strip()        \n",
    "        \n",
    "        chap_info['chapters_original_current'] = chapter_current if chapter_current != \"\" else None \n",
    "    chap_info['complete_translated'] = get_bool(get_value(content.find('div', attrs={'id': 'showtranslated'})))\n",
    "    \n",
    "    table = content.find('table', attrs={'id': 'myTable'})\n",
    "    if table is not None:\n",
    "        release_table = table.find('tbody')\n",
    "        chap_info['chapter_latest_translated'] = release_table.find('tr').find_all('td')[2].a.string.strip()\n",
    "    return chap_info\n",
    "    \n",
    "    \n",
    "def release_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all release and activity information of a specific novel.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    rel_info = {}\n",
    "    release_freq = content.find('h5', attrs={'class': 'seriesother'}, string='Release Frequency').next_sibling\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    \n",
    "    if not empty(release_freq):\n",
    "        rel_info['release_freq'] = float(re.search('\\d+\\.?\\d*', release_freq).group(0))\n",
    "        \n",
    "    rel_info['activity_week_rank'] = int(activity[0].string[1:])\n",
    "    rel_info['activity_month_rank'] = int(activity[1].string[1:])\n",
    "    rel_info['activity_all_time_rank'] = int(activity[2].string[1:])\n",
    "    return rel_info\n",
    "    \n",
    "\n",
    "def community_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all community information of a specific novels.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    comm_info = {}\n",
    "    activity = content.find_all('span', attrs={'class': 'userrate rank'})\n",
    "    comm_info['on_reading_lists'] = int(content.find('b', attrs={'class': 'rlist'}).string)\n",
    "    comm_info['reading_list_month_rank'] = int(activity[3].string[1:])\n",
    "    comm_info['reading_list_all_time_rank'] = int(activity[4].string[1:])\n",
    "    \n",
    "    # rating\n",
    "    rating_text = content.find('span', attrs={'class': 'uvotes'}).text.split(' ')\n",
    "    comm_info['rating'] = float(rating_text[0][1:])\n",
    "    comm_info['rating_votes'] = int(rating_text[3])\n",
    "    return comm_info\n",
    "    \n",
    "    \n",
    "def relation_info(content):\n",
    "    \"\"\"\n",
    "    Scrapes all relational information of a specific novel.\n",
    "    \n",
    "    :param content: The content page of a novel.\n",
    "    :returns: A dictionary with scraped and cleaned information.\n",
    "    \"\"\"\n",
    "    rel_info = {}\n",
    "    two_third_content = content.find('div', attrs={'class': 'two-thirds'})\n",
    "    wpb_wrapper = two_third_content.find('div', attrs={'class': 'wpb_wrapper'})\n",
    "    \n",
    "    rel_info['related_series_ids'] = []\n",
    "    rel_info['recommended_series_ids'] = []\n",
    "    rel_info['recommendation_list_ids'] = []\n",
    "    for series in wpb_wrapper.findChildren('a', attrs={'class': 'genre'}, recursive=False):\n",
    "        if series.has_attr('title'):\n",
    "            rel_info['recommended_series_ids'].append(series.get('id')[3:])\n",
    "        else:\n",
    "            rel_info['related_series_ids'].append(series.get('id')[3:])\n",
    "            \n",
    "    rec_lists = wpb_wrapper.find('ol', attrs={'class': 'ulc_sp'})\n",
    "    if rec_lists:\n",
    "        rel_info['recommendation_list_ids'] = [int(a['href'].split('/')[-2]) \n",
    "                                               for a in rec_lists.findChildren('a')]\n",
    "    \n",
    "    # Return None in the cases where nothing is found (and not []).\n",
    "    rel_info.update((k, None) for k, v in rel_info.items() if len(v) == 0)\n",
    "    return rel_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_novel_page(novel_id):\n",
    "    \"\"\"\n",
    "    Parses and scrapes information from a single novel page.\n",
    "    \n",
    "    :param novel_id: The id number of the novel.\n",
    "    :returns: A pandas series with all scraped and cleaned information about the novel.\n",
    "    \"\"\"\n",
    "    \n",
    "    page = requests.get(novel_page + str(novel_id))    \n",
    "    soup = BeautifulSoup(page.content, 'html5lib')\n",
    "    content = soup.find('div', attrs={'class': 'w-blog-content'})\n",
    "    if content is None:\n",
    "        return pd.Series() \n",
    "    \n",
    "    data = {'id': novel_id}\n",
    "    data.update(general_info(content))\n",
    "    data.update(publisher_info(content))\n",
    "    data.update(chapter_info(content))\n",
    "    data.update(release_info(content))\n",
    "    data.update(community_info(content))\n",
    "    data.update(relation_info(content))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = df['id'].apply(lambda novel_id: parse_novel_page(novel_id))\n",
    "df.to_csv('novels.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
